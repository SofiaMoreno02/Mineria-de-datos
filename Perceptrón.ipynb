{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfde269-ef5b-41cb-8cbd-21188841b0c0",
   "metadata": {},
   "source": [
    "<center><strong><h1 style=\"color:red;\">Redes neuronales: Perceptrón</h1></strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7aa06-ecf6-4df9-b148-a71a99615fa9",
   "metadata": {},
   "source": [
   "<center>\n",
   "<figure>\n",
   "<img src=\"https://raw.githubusercontent.com/SofiaMoreno02/Mineria-de-datos/main/Imagenes/ai-gf3d9aa133_1920.jpg\" align=\"center\" /> \n",
   "</figure>\n",
   "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2a010-a5f2-43a4-ae44-1b8c7a0b013c",
   "metadata": {},
   "source": [
    "Fuente: [Imagénes libres en pixabay](https://pixabay.com/es/illustrations/ai-inteligencia-artificial-7114793/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98824daf-18da-4c3b-a395-2168c55692bb",
   "metadata": {},
   "source": [
    "<left><strong><h2 style=\"color:blue;\">Autores</h2></strong></left>\n",
    "* Ariadna Sofia Contreras Abril, arcontrerasa@unal.edu.co\n",
    "* Julieth Sofia Moreno Ahumada, jmorenoah@unal.edu.co\n",
    "* Nicolás Suárez Gutiérrez, nisuarez@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b4cfa-67ee-4723-8201-e5d025dc1e2e",
   "metadata": {},
   "source": [
    "<left><strong><h2 style=\"color:blue;\">Referencias</h2></strong></left>\n",
    "* [Flexible statistical inference for mechanistic models of neural dynamics, J-M Lueckmann](https://proceedings.neurips.cc/paper/2017/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf)\n",
    "* [Neural Networks and Statistical, ModelsWarren S. Sarle](https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=0956C808E6C23BF0414809E3AA6B0757?doi=10.1.1.27.699&rep=rep1&type=pdf)\n",
    "* [IEE.EXPLORE](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=471372)\n",
    "* [Redes Neuronales,\n",
    "Jamie Areli-Toral Barrera](http://www.cucei.udg.mx/sites/default/files/pdf/toral_barrera_jamie_areli.pdf)\n",
    "\n",
    "* [Neural Network Toolbox™ 6 User’s Guide, H Demuth, M Beale,M Hagan](https://kashanu.ac.ir/Files/Content/neural_network_toolbox_6.pdf) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d254b6-693d-43c6-8083-a877bbb3677d",
   "metadata": {},
   "source": [
    "<left><strong><h2 style=\"color:blue;\">Introducción</h2></strong></left>\n",
    "\n",
    "Las redes neuronales son una herramienta computacional, basadas en modelos matemáticos que intentan imitar el funcionamiento de las neuronas del cerebro humano. En la actualidad estas herramientas han ganado cada vez mayor reconocimiento debido a su gran aplicabilidad a la resolución de problemas de regresión, clasificación y clustering en los que la estadística tradicional no brinda suficientes herramientas.\n",
    "\n",
    "Debido al constante avance de la tecnología y la aparición de problemas nuevos en cuanto al tratamiento y procesamiento de la información es que las redes neuronales se han venido desarrollado constantemente, permitiendo así que existan diversos tipos de estructuras para una red neuronal, las cuáles dependen del propósito con el cual se desea construir dicha red. Estas estructuras al igual que las redes neuronales biológicas constan de múltiples neuronas interconectadas. Algunos ejemplos de estructuras son:\n",
    "\n",
    "* Perceptrón\n",
    "* Auto-encoder\n",
    "* Red recurrente\n",
    "* Red convolucional\n",
    "\n",
    "\n",
    "A partir de esto podemos entender un poco mejor que es un perceptrón."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095c56f-3f76-4b7d-9501-6e9f22344340",
   "metadata": {},
   "source": [
    "<left><strong><h2 style=\"color:blue;\">Reseña histórica</h2></strong></left>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb9f30-dff6-4b4c-85b5-fb55f03e58c0",
   "metadata": {},
   "source": [
    "<img src=\"Rosenblatt_21.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3260ceb-4d91-4139-ae5b-77684a2bbd96",
   "metadata": {},
   "source": [
    "Fuente: [Wikipedia](https://es.wikipedia.org/wiki/Frank_Rosenblatt#/media/Archivo:Rosenblatt_21.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a995ad-c0c5-40fe-81cb-85feb0ba1e16",
   "metadata": {},
   "source": [
    "Frank Rosenblatt fue un psicólogo estadounidense, considerado en la actualidad como el padre del Deep learning. Nació el 11 de julio de 1928 en Nueva York, estudió psicología en el instituto Bronx de Ciencia, posteriormente obtuvo la licenciatura en letras y un doctorado en filosofía. Trabajó en el Laboratorio Cornell Aeronáutico donde se desempeñó como jefe de la sección de sistemas cognitivos. Durante su vida se dedicó a la enseñanza e investigación en psicología, sistemas cognitivos y transferencia del comportamiento. Falleció el 11 de julio de 1971, tras su muerte las investigaciones en redes neuronales se detuvieron hasta la época de los 80 cuando nuevos investigadores retomaron el trabajo de Rosenblatt.\n",
    "\n",
    "El trabajo de Frank Rosenblatt, fue el principal referente del “Mark I perceptrón” el primer computador diseñado para crear redes neuronales artificiales, podemos clasificarlo como deep learning pues no hacía falta ponerle parámetros preestablecidos, pues el mismo programa construye sus parámetros cada vez que realiza una repetición del experimento, la función del mark I es descifrar el sexo de rostros humanos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f69c158-3574-4041-9f29-80423b85c56e",
   "metadata": {},
   "source": [
    "<left><h2 style=\"color:#4361EE;\">¿Qué es un perceptrón?</h2></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b9d99-f13e-4d2f-a39d-53bec7ec1c1d",
   "metadata": {},
   "source": [
    "<img src=\"Imagen2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ae3bb-59f3-4114-ba98-d875a4836a2e",
   "metadata": {},
   "source": [
    "Fuente: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Perceptr%C3%B3n_5_unidades.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a52dce-fb1c-4cef-b7d9-09bb9c553397",
   "metadata": {},
   "source": [
    "Como lo mencionamos anteriormente un perceptrón es en principio una estructura de red neuronal. Como vemos en el gráfico anterior, un perceptrón funciona como una neurona, pues recibe unas señales de entrada, las cuales pondera por los pesos sinápticos y las suma, posteriormente la función de activación puede entenderse como un umbral que dicha suma de señales debe superar para lograr \"encender\" la neurona, asi como sucede con las neuronas biológicas, pues los impulsos que resiven deben ser suficientemente fuertes para activar la sinapsis y asi reenviar ese impulso a las siguientes neuronas conectadas.\n",
    "\n",
    "A partir de esto podemos resumir el perceptrón como una función matemática que devuelve como resultado 0's y 1's, entonces se puede entender como un algoritmo clasificador binario, el cual puede hacer una predicción binaria con base a unos valores de entrada y un aprendizaje previo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580dcdb-06b4-436b-9906-f843e9f85d7d",
   "metadata": {},
   "source": [
    "<left><strong><h2 style=\"color:blue;\">Fundamentación matemática</h2></strong></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdede2f-2753-4fd6-840d-5c094aaf9234",
   "metadata": {},
   "source": [
    "<left><strong><h3 style=\"color:#4CC9F0;\"> Funcionamiento general </h3></strong></left>\n",
    "\n",
    " El perceptrón es muy bueno resolviendo con funciones lineales $(0, 1)$ pero es muy débil para la construcción y predicción con modelos no lineales. Para su correcto funcionamiento hace falta, a parte programarlo, entrenarlo.\n",
    "\n",
    "Las partes de un perceptrón son las siguientes:\n",
    "\n",
    "**1.Conjunto de entradas:** Dependiendo del modelo las entradas pueden ser binarias o continuas, cada valor se guarda en un $x_j$ vector de entradas tamaño $n$.\n",
    "\n",
    "**2.Pesos sinápticos  asociados a las entradas:** Representan la intensidad de interacción $W_i$ entre cada neurona presináptica j y la neurona postsináptica i, el producto sera un $W_{ij}$ estos son guardados en una Matriz $W$.\n",
    "\n",
    "$z_{i}=x_{i}W+b_{i}$ donde b son interceptos del vector $x_{i}$ podemos construir una matrizde bloques para simplificar la función, pero el resultado es el mismo.\n",
    "\n",
    "**3.Regla de propagación:** Proporciona el valor del potencial postsináptico, h(t), de la neurona i en función de sus pesos y entradas.\n",
    "\n",
    "**4.Función de activación que representa simultáneamente la salida de la neurona y su estado de activación:** Proporciona el estado de activación actual, $a$, de la neurona $i$ en función de su estado anterior, $a$, y de su potencial postsináptico actual. En muchos modelos de ANS se considera que el estado actual de la neurona no depende de su estado anterior, sino únicamente del actual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8159b2a4-0247-406a-80a5-1bd624f89dad",
   "metadata": {},
   "source": [
    "<left><strong><h2 style=\"color:blue;\"> Implementación </h2></strong></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c4b21-5df4-4250-86cb-5d74b29933fa",
   "metadata": {},
   "source": [
    "<left><strong><h2 style=\"color:blue;\"> Ejemplos aplicados </h2></strong></left>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
