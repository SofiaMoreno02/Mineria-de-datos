{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><strong><h1 style=\"color:red;\">Redes neuronales: Perceptrón</h1></strong></center>\n",
        "<center><strong><h2 style=\"color:black;\"> Universidad Nacional de Colombia </h2></strong></center>\n",
        "<center><strong><h5 style=\"color:black;\"> Ariadna Sofia Contreras Abril </h5></strong></center>\n",
        "<center><strong><h5 style=\"color:black;\"> Julieth Sofia Moreno Ahumada </h5></strong></center>\n",
        "<center><strong><h5 style=\"color:black;\"> Nicolás Suárez Gutiérrez </h5></strong></center>"
      ],
      "metadata": {
        "id": "eDf_dye5Bm3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<left><strong><h2 style=\"color:blue;\"> Introducción </h2></strong></left> \n",
        "<p align=\"justify\">El deep learning es un campo del machine learning que entrena a una computadora para que realice tareas de predicción en lugar de organizar datos para que se ejecuten a través de ecuaciones predefinidas, el deep learning configura parámetros básicos acerca de los datos estructurados o no, y entrena a la computadora para que aprenda por cuenta propia reconociendo patrones mediante el uso de muchas capas de procesamiento. Esta es la base de la Inteligencia Artificial (IA).</p>\n",
        "<p align=\"justify\">El perceptrón entonces se puede entender como un algoritmo clasificador binario, el cual puede hacer una predicción binaria con base a unos valores de entrada y un aprendizaje previo.</p>"
      ],
      "metadata": {
        "id": "pmY5e_5rCfbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<left><strong><h2 style=\"color:blue;\"> Reseña Histórica </h2></strong></left> \n",
        "<p align=\"justify\"> Frank Rosenblatt fue un psicólogo estadounidense, considerado en la actualidad como el padre del Deep learning. Nació el 11 de julio de 1928 en Nueva York, estudió psicología en el instituto Bronx de Ciencia, posteriormente obtuvo la licenciatura en letras y un doctorado en filosofía. Trabajó en el Laboratorio Cornell Aeronáutico donde se desempeñó como jefe de la sección de sistemas cognitivos. Durante su vida se dedicó a la enseñanza e investigación en psicología, sistemas cognitivos y transferencia del comportamiento. Falleció el 11 de julio de 1971,  tras su muerte las investigaciones en redes neuronales se detuvieron hasta la época de los 80 cuando nuevos investigadores retomaron el trabajo de Rosenblatt.\n",
        "</p>\n",
        "<p align=\"justify\">El trabajo de Frank Rosenblatt, fue el principal referente del “Mark I perceptrón” el primer computador diseñado para crear redes neuronales artificiales, podemos clasificarlo como deep learning pues no hacía falta ponerle parámetros preestablecidos, pues el mismo programa construye sus parámetros cada vez que realiza una repetición del experimento, la función del mark I es descifrar el sexo de rostros humanos.</p>"
      ],
      "metadata": {
        "id": "SYx_CyoCCsdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<left><strong><h2 style=\"color:blue;\"> Fundamentación matemática </h2></strong></left>\n",
        "\n",
        "<left><strong><h3 style=\"color:#4CC9F0;\"> Funcionamiento general </h3></strong></left>\n",
        "\n",
        " El perceptrón es muy bueno resolviendo con funciones lineales $(0, 1)$ pero es muy débil para la construcción y predicción con modelos no lineales. Para su correcto funcionamiento hace falta, a parte programarlo, entrenarlo.\n",
        "\n",
        "Las partes de un perceptrón son las siguientes:\n",
        "\n",
        "**1.Conjunto de entradas:** Dependiendo del modelo las entradas pueden ser binarias o continuas, cada valor se guarda en un $x_j$.\n",
        "\n",
        "**2.Pesos sinápticos  asociados a las entradas:** Representan la intensidad de interacción $W_i$ entre cada neurona presináptica j y la neurona postsináptica i, el producto sera un $W_{ij}$.\n",
        "\n",
        "**3.Regla de propagación:** Proporciona el valor del potencial postsináptico, h(t), de la neurona i en función de sus pesos y entradas.\n",
        "\n",
        "**4.Función de activación que representa simultáneamente la salida de la neurona y su estado de activación:** Proporciona el estado de activación actual, $a$, de la neurona $i$ en función de su estado anterior, $a$, y de su potencial postsináptico actual. En muchos modelos de ANS se considera que el estado actual de la neurona no depende de su estado anterior, sino únicamente del actual.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vc8ax-TqDGD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<left><strong><h2 style=\"color:#0F76FF;\"> Implementación </h2></strong></left>"
      ],
      "metadata": {
        "id": "ka1JBKqnEntW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<left><strong><h2 style=\"color:#0F76FF;\"> Ejemplos aplicados </h2></strong></left>\n",
        "\n",
        "<left><strong><h4 style=\"color:black;\"> Regresión no lineal </h4></strong></left>\n",
        "\n",
        "<left><strong><h4 style=\"color:black;\"> Clasificación no lineal </h4></strong></left>"
      ],
      "metadata": {
        "id": "rP26OFFcEv3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento de un  perceptrón simple.\n",
        "\n",
        "un metodo mas actualizzado y practico son los\n",
        "Metodos de convergencia, utilizando metodos numericos, pero no es el caso.\n",
        "\n",
        "se usa el método Supervisado.\n",
        "\n",
        "para que el software pueda reconocer los datos hacia falta de un entrenamiento en el consiste mostrar las entradas y el resultado del trayecto, corregirlo o aprobarlo, por un asistente humano, existen funciones de corregimiento para modificar los pesos sinapticos asociados y llevar cada vez a resultados más acertados."
      ],
      "metadata": {
        "id": "JHn0rA8cz783"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<left><strong><h2 style=\"color:#0F76FF;\"> Referencias </h2></strong></left>\n",
        "* Flexible statistical inference for mechanistic models of\n",
        "neural dynamics, J-M Lueckmann:\n",
        "https://proceedings.neurips.cc/paper/2017/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf\n",
        "\n",
        "* Neural Networks and Statistical, ModelsWarren S. Sarle:\n",
        "https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=0956C808E6C23BF0414809E3AA6B0757?doi=10.1.1.27.699&rep=rep1&type=pdf\n",
        "\n",
        "* IEE.EXPLORE:\n",
        " https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=471372\n",
        "\n",
        "\n",
        "* Redes Neuronales,\n",
        "Jamie Areli-Toral Barrera:\n",
        "http://www.cucei.udg.mx/sites/default/files/pdf/toral_barrera_jamie_areli.pdf\n",
        "\n",
        "* Neural Network Toolbox™ 6\n",
        "User’s Guide,\n",
        "H Demuth,\n",
        "M Beale,\n",
        "M Hagan,\n",
        "https://kashanu.ac.ir/Files/Content/neural_network_toolbox_6.pdf \n",
        "\n"
      ],
      "metadata": {
        "id": "iNBWMSYSKe-h"
      }
    }
  ]
}